{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heejipark/leetcode/blob/master/edit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1YpElo3DVZ7",
        "outputId": "2d476df3-7794-4feb-bebd-bd80d47e0fa6"
      },
      "id": "r1YpElo3DVZ7",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "753c7be0",
      "metadata": {
        "id": "753c7be0"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vitaldb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOWCfVrjEXDb",
        "outputId": "5e327732-4f52-404f-db39-3ad015e091f9"
      },
      "id": "bOWCfVrjEXDb",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: vitaldb in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from vitaldb) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vitaldb) (2.23.0)\n",
            "Requirement already satisfied: s3fs in /usr/local/lib/python3.7/dist-packages (from vitaldb) (2022.5.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from vitaldb) (1.3.5)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from vitaldb) (6.0.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->vitaldb) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->vitaldb) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->vitaldb) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vitaldb) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vitaldb) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vitaldb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vitaldb) (3.0.4)\n",
            "Requirement already satisfied: fsspec==2022.5.0 in /usr/local/lib/python3.7/dist-packages (from s3fs->vitaldb) (2022.5.0)\n",
            "Requirement already satisfied: aiohttp<=4 in /usr/local/lib/python3.7/dist-packages (from s3fs->vitaldb) (3.8.1)\n",
            "Requirement already satisfied: aiobotocore~=2.3.0 in /usr/local/lib/python3.7/dist-packages (from s3fs->vitaldb) (2.3.4)\n",
            "Requirement already satisfied: wrapt>=1.10.10 in /usr/local/lib/python3.7/dist-packages (from aiobotocore~=2.3.0->s3fs->vitaldb) (1.14.1)\n",
            "Requirement already satisfied: botocore<1.24.22,>=1.24.21 in /usr/local/lib/python3.7/dist-packages (from aiobotocore~=2.3.0->s3fs->vitaldb) (1.24.21)\n",
            "Requirement already satisfied: aioitertools>=0.5.1 in /usr/local/lib/python3.7/dist-packages (from aiobotocore~=2.3.0->s3fs->vitaldb) (0.10.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp<=4->s3fs->vitaldb) (1.2.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp<=4->s3fs->vitaldb) (2.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp<=4->s3fs->vitaldb) (6.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp<=4->s3fs->vitaldb) (21.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp<=4->s3fs->vitaldb) (4.1.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp<=4->s3fs->vitaldb) (1.7.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp<=4->s3fs->vitaldb) (1.3.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp<=4->s3fs->vitaldb) (4.0.2)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp<=4->s3fs->vitaldb) (0.13.0)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.24.22,>=1.24.21->aiobotocore~=2.3.0->s3fs->vitaldb) (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e1f1b4dd",
      "metadata": {
        "id": "e1f1b4dd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Check time\n",
        "from tqdm import tqdm\n",
        "from time import sleep\n",
        "\n",
        "# Datasets\n",
        "import vitaldb\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "from torch.optim import Adam\n",
        "import torch.utils.data\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torchvision.datasets\n",
        "import torchvision.utils as vutils\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# For model\n",
        "import argparse\n",
        "import itertools\n",
        "\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "cachefile = '/content/drive/MyDrive/snuh/ppg_abp_datasets_peak_50_with_filter_ppgabp.npz'\n",
        "\n",
        "# set the computation device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.Tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "af68d7c3",
      "metadata": {
        "id": "af68d7c3"
      },
      "outputs": [],
      "source": [
        "ppg_abp_sets = np.load(cachefile)       # Load cache file\n",
        "ppg_sets = ppg_abp_sets['ppg_sets']\n",
        "abp_sets = ppg_abp_sets['abp_sets']\n",
        "ppg_abp_sets.close()                    # For memory efficiency, close the file"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Shape of data ----------------------------\n",
        "print(ppg_sets.shape)\n",
        "print(abp_sets.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRAEzert-k9S",
        "outputId": "6810ed84-67a1-4f40-fcf6-68ae6a7a64a5"
      },
      "id": "bRAEzert-k9S",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(12738, 2000)\n",
            "(12738, 2000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "308dd67c",
      "metadata": {
        "id": "308dd67c"
      },
      "source": [
        "## Hyperparameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "381b4bda",
      "metadata": {
        "id": "381b4bda"
      },
      "outputs": [],
      "source": [
        "# Data path\n",
        "epoch = 0            # starting epoch\n",
        "n_epochs = 10        # number of epochs of training\n",
        "batch_size = 64       # size of the batches\n",
        "lr = 1e-4            # initial learning rate\n",
        "decay_epoch = 100    # epoch to start linearly decaying the learning rate to 0\n",
        "workers = 2          # number of workers\n",
        "input_nc = 1         # number of channels of input data\n",
        "output_nc = 1        # number of channels of output data\n",
        "beta1 = 1            # beta1 value for Adam optimizer\n",
        "seed = 30            # random seed\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8c8e6f1",
      "metadata": {
        "id": "c8c8e6f1"
      },
      "source": [
        "## Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Find minimum and maximum values\n",
        "def findMinMax(datasets):\n",
        "  return [np.max(datasets), np.min(datasets)]\n",
        "\n",
        "# Min-Max Normalization\n",
        "def minMax(datasets):\n",
        "  maxVal, minVal = findMinMax(datasets)[0], findMinMax(datasets)[1]\n",
        "  normVal = (datasets - minVal) / (maxVal - minVal)\n",
        "  return normVal\n",
        "\n",
        "# weights initalization\n",
        "def weights_init_normal(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm1d') != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        torch.nn.init.constant(m.bias.data, 0.0)"
      ],
      "metadata": {
        "id": "Y85ouzzwpu1Z"
      },
      "id": "Y85ouzzwpu1Z",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ebc6e708",
      "metadata": {
        "id": "ebc6e708"
      },
      "outputs": [],
      "source": [
        "# Split data into training and testing datasets\n",
        "ppg_train, ppg_test, abp_train, abp_test = train_test_split(ppg_sets, abp_sets, test_size=0.2, random_state=25)\n",
        "\n",
        "## Normalization\n",
        "norm_ppg_train, norm_abp_train = minMax(ppg_train), minMax(abp_train)\n",
        "norm_ppg_test, norm_abp_test = minMax(ppg_test), minMax(abp_test)\n",
        "\n",
        "## Change data type\n",
        "norm_ppg_train = torch.tensor(norm_ppg_train, dtype=torch.float32)\n",
        "norm_abp_train = torch.tensor(norm_abp_train, dtype=torch.float32)\n",
        "norm_ppg_test = torch.tensor(norm_ppg_test, dtype=torch.float32)\n",
        "norm_abp_test = torch.tensor(norm_abp_test, dtype=torch.float32)\n",
        "\n",
        "## Make the datasets as a TensorDataset\n",
        "ds_train = TensorDataset(norm_ppg_train, norm_abp_train)\n",
        "ds_test = TensorDataset(norm_ppg_test, norm_abp_test)\n",
        "\n",
        "# Create the dataloader\n",
        "loader_train = DataLoader(ds_train, batch_size=batch_size, shuffle=True, num_workers=workers)\n",
        "loader_test = DataLoader(ds_test, batch_size=batch_size, shuffle=False, num_workers=workers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "08a1a2b2",
      "metadata": {
        "id": "08a1a2b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9814a240-33c4-44a9-8db6-e3cc96e4115b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10190, 2000)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "ppg_train.shape # (batch size, dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f46285f",
      "metadata": {
        "id": "8f46285f"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "c8cb84a5",
      "metadata": {
        "id": "c8cb84a5"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_features):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        layer = [nn.ReflectionPad1d(1),\n",
        "                 nn.Conv1d(in_features, in_features, 3),\n",
        "                 nn.InstanceNorm1d(in_features),\n",
        "                 nn.ReLU(inplace=True),\n",
        "                 nn.ReflectionPad1d(1),\n",
        "                 nn.Conv1d(in_features, in_features, 3),\n",
        "                 nn.InstanceNorm1d(in_features)]\n",
        "        \n",
        "        self.conv_block = nn.Sequential(*layer)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.conv_block(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "413d9172",
      "metadata": {
        "id": "413d9172"
      },
      "source": [
        "1. Generator(Encoder)\n",
        "    -> C64, C128, C256, C512, C512, C512, C512, C512\n",
        "2. Generator(Decoder)\n",
        "    -> CD512 - CD \n",
        "3. Discriminator\n",
        "    -> C64 - C128 - C256 - C512 - C1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "9d08b4ba",
      "metadata": {
        "id": "9d08b4ba"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_nc, output_nc, n_residual_blocks=9):\n",
        "        super(Generator, self).__init__()\n",
        "    \n",
        "        # Initial convolution block       \n",
        "        self.model1 = nn.Sequential(\n",
        "                 nn.ReflectionPad1d(3),\n",
        "                 nn.Conv1d(input_nc, 64, 7),\n",
        "                 nn.InstanceNorm1d(64),\n",
        "                 nn.ReLU(inplace=True)\n",
        "                 )\n",
        "\n",
        "        # Encoder - Downsampling\n",
        "        in_features = 64\n",
        "        out_features = in_features*2\n",
        "        model2 = []\n",
        "        for _ in range(2):\n",
        "            model2 += [nn.Conv1d(in_features, out_features, kernel_size=3, stride=2, padding=1),\n",
        "                       nn.InstanceNorm1d(out_features),\n",
        "                       nn.ReLU(inplace=True)]\n",
        "            in_features = out_features\n",
        "            out_features = in_features*2\n",
        "        self.model2 = nn.Sequential(*model2)\n",
        "        \n",
        "        model3=[]\n",
        "        # Residual blocks\n",
        "        for _ in range(n_residual_blocks):\n",
        "            model3 += [ResidualBlock(in_features)]\n",
        "        self.model3 = nn.Sequential(*model3)\n",
        "        \n",
        "        # Decoder - Upsampling\n",
        "        model4 = []\n",
        "        out_features = in_features//2\n",
        "        for _ in range(2):\n",
        "            model4 += [nn.ConvTranspose1d(in_features, out_features, 3, stride=2, padding=1, output_padding=1),\n",
        "                      nn.InstanceNorm1d(out_features),\n",
        "                      nn.ReLU(inplace=True)]\n",
        "            in_features = out_features\n",
        "            out_features = in_features//2\n",
        "        self.model4 = nn.Sequential(*model4)\n",
        "        \n",
        "        # Output layer\n",
        "        self.model5 = nn.Sequential(\n",
        "                  nn.ReflectionPad1d(3),\n",
        "                  nn.Conv1d(64, output_nc, 7),\n",
        "                  nn.Tanh())\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model1(x) \n",
        "        x = self.model2(x)\n",
        "        x = self.model3(x)\n",
        "        x = self.model4(x)\n",
        "        out = self.model5(x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "7f9cb7f5",
      "metadata": {
        "id": "7f9cb7f5"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_nc):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        # A bunch of convolutions one after another\n",
        "        model = [nn.Conv1d(input_nc, 64, 4, stride=2, padding=1),\n",
        "                 nn.LeakyReLU(0.2, inplace=True) ]\n",
        "\n",
        "        model += [nn.Conv1d(64, 128, 4, stride=2, padding=1),\n",
        "                  nn.InstanceNorm1d(128), \n",
        "                  nn.LeakyReLU(0.2, inplace=True)]\n",
        "\n",
        "        model += [nn.Conv1d(128, 256, 4, stride=2, padding=1),\n",
        "                  nn.InstanceNorm1d(256), \n",
        "                  nn.LeakyReLU(0.2, inplace=True)]\n",
        "\n",
        "        model += [nn.Conv1d(256, 512, 4, padding=1),\n",
        "                  nn.InstanceNorm1d(512), \n",
        "                  nn.LeakyReLU(0.2, inplace=True)]\n",
        "\n",
        "        # FCN classification layer\n",
        "        model += [nn.Conv1d(512, 1, 4, padding=1)]\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        # Average pooling and flatten\n",
        "        return F.avg_pool1d(x, x.size()[2:]).view(x.size()[0], -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "e6b5c62d",
      "metadata": {
        "id": "e6b5c62d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a42bbc9b-c711-44f4-9bdc-74bdd5b280b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ]
        }
      ],
      "source": [
        "# Initalize Generator and Discriminator\n",
        "netG_P2A = Generator(input_nc, output_nc).to(device)\n",
        "netG_A2P = Generator(output_nc, input_nc).to(device)\n",
        "netD_PPG = Discriminator(input_nc).to(device)\n",
        "netD_ABP = Discriminator(input_nc).to(device)\n",
        "\n",
        "def weights_init_normal(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        torch.nn.init.normal(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm1d') != -1:\n",
        "        torch.nn.init.normal(m.weight.data, 1.0, 0.02)\n",
        "        torch.nn.init.constant(m.bias.data, 0.0)\n",
        "\n",
        "\n",
        "# need Weight Initialization\n",
        "netG_P2A.apply(weights_init_normal)\n",
        "netG_A2P.apply(weights_init_normal)\n",
        "netD_PPG.apply(weights_init_normal)\n",
        "netD_ABP.apply(weights_init_normal)\n",
        "\n",
        "# Optimizers\n",
        "optimizer_G = optim.Adam(itertools.chain(netG_P2A.parameters(), netG_A2P.parameters()), lr=lr, betas=(0.5, 0.999))\n",
        "optimizer_D_PPG = optim.Adam(netD_ABP.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "optimizer_D_ABP = optim.Adam(netD_PPG.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "\n",
        "# Cross-function\n",
        "criterion_gan = nn.MSELoss().to(device)      # How the discriminator discriminates the data whether it is generated or not\n",
        "criterion_identity = nn.L1Loss().to(device)  # How different the generated data is from the original data through forward OR backward transformations\n",
        "criterion_cycle = nn.L1Loss().to(device)     # How different the generated data is from the original data through forward AND backward transformations\n",
        "\n",
        "\n",
        "## learning rate schedulers\n",
        "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(optimizer_G, lr_lambda=lambda epoch: 0.95 ** epoch)\n",
        "lr_scheduler_D_PPG = torch.optim.lr_scheduler.LambdaLR(optimizer_D_PPG, lr_lambda=lambda epoch: 0.95 ** epoch)\n",
        "lr_scheduler_D_ABP = torch.optim.lr_scheduler.LambdaLR(optimizer_D_ABP, lr_lambda=lambda epoch: 0.95 ** epoch)\n",
        "\n",
        "\n",
        "# Initaliza loss value\n",
        "min_loss_G = float('inf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "d4b9fb67",
      "metadata": {
        "scrolled": true,
        "id": "d4b9fb67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5b8ff11-a1f9-427f-a13b-39039ad0e6d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator(\n",
            "  (model1): Sequential(\n",
            "    (0): ReflectionPad1d((3, 3))\n",
            "    (1): Conv1d(1, 64, kernel_size=(7,), stride=(1,))\n",
            "    (2): InstanceNorm1d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (3): ReLU(inplace=True)\n",
            "  )\n",
            "  (model2): Sequential(\n",
            "    (0): Conv1d(64, 128, kernel_size=(3,), stride=(2,), padding=(1,))\n",
            "    (1): InstanceNorm1d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv1d(128, 256, kernel_size=(3,), stride=(2,), padding=(1,))\n",
            "    (4): InstanceNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (5): ReLU(inplace=True)\n",
            "  )\n",
            "  (model3): Sequential(\n",
            "    (0): ResidualBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad1d((1, 1))\n",
            "        (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,))\n",
            "        (2): InstanceNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad1d((1, 1))\n",
            "        (5): Conv1d(256, 256, kernel_size=(3,), stride=(1,))\n",
            "        (6): InstanceNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (1): ResidualBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad1d((1, 1))\n",
            "        (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,))\n",
            "        (2): InstanceNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad1d((1, 1))\n",
            "        (5): Conv1d(256, 256, kernel_size=(3,), stride=(1,))\n",
            "        (6): InstanceNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (2): ResidualBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad1d((1, 1))\n",
            "        (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,))\n",
            "        (2): InstanceNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad1d((1, 1))\n",
            "        (5): Conv1d(256, 256, kernel_size=(3,), stride=(1,))\n",
            "        (6): InstanceNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (3): ResidualBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad1d((1, 1))\n",
            "        (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,))\n",
            "        (2): InstanceNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad1d((1, 1))\n",
            "        (5): Conv1d(256, 256, kernel_size=(3,), stride=(1,))\n",
            "        (6): InstanceNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (4): ResidualBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad1d((1, 1))\n",
            "        (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,))\n",
            "        (2): InstanceNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad1d((1, 1))\n",
            "        (5): Conv1d(256, 256, kernel_size=(3,), stride=(1,))\n",
            "        (6): InstanceNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (5): ResidualBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad1d((1, 1))\n",
            "        (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,))\n",
            "        (2): InstanceNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad1d((1, 1))\n",
            "        (5): Conv1d(256, 256, kernel_size=(3,), stride=(1,))\n",
            "        (6): InstanceNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (6): ResidualBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad1d((1, 1))\n",
            "        (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,))\n",
            "        (2): InstanceNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad1d((1, 1))\n",
            "        (5): Conv1d(256, 256, kernel_size=(3,), stride=(1,))\n",
            "        (6): InstanceNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (7): ResidualBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad1d((1, 1))\n",
            "        (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,))\n",
            "        (2): InstanceNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad1d((1, 1))\n",
            "        (5): Conv1d(256, 256, kernel_size=(3,), stride=(1,))\n",
            "        (6): InstanceNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (8): ResidualBlock(\n",
            "      (conv_block): Sequential(\n",
            "        (0): ReflectionPad1d((1, 1))\n",
            "        (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,))\n",
            "        (2): InstanceNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "        (3): ReLU(inplace=True)\n",
            "        (4): ReflectionPad1d((1, 1))\n",
            "        (5): Conv1d(256, 256, kernel_size=(3,), stride=(1,))\n",
            "        (6): InstanceNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (model4): Sequential(\n",
            "    (0): ConvTranspose1d(256, 128, kernel_size=(3,), stride=(2,), padding=(1,), output_padding=(1,))\n",
            "    (1): InstanceNorm1d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): ConvTranspose1d(128, 64, kernel_size=(3,), stride=(2,), padding=(1,), output_padding=(1,))\n",
            "    (4): InstanceNorm1d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (5): ReLU(inplace=True)\n",
            "  )\n",
            "  (model5): Sequential(\n",
            "    (0): ReflectionPad1d((3, 3))\n",
            "    (1): Conv1d(64, 1, kernel_size=(7,), stride=(1,))\n",
            "    (2): Tanh()\n",
            "  )\n",
            ")\n",
            "Discriminator(\n",
            "  (model): Sequential(\n",
            "    (0): Conv1d(1, 64, kernel_size=(4,), stride=(2,), padding=(1,))\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (2): Conv1d(64, 128, kernel_size=(4,), stride=(2,), padding=(1,))\n",
            "    (3): InstanceNorm1d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (5): Conv1d(128, 256, kernel_size=(4,), stride=(2,), padding=(1,))\n",
            "    (6): InstanceNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (8): Conv1d(256, 512, kernel_size=(4,), stride=(1,), padding=(1,))\n",
            "    (9): InstanceNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (11): Conv1d(512, 1, kernel_size=(4,), stride=(1,), padding=(1,))\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(netG_P2A)\n",
        "print(netD_PPG)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bcb8d8d",
      "metadata": {
        "id": "1bcb8d8d"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab29a14e",
      "metadata": {
        "scrolled": true,
        "id": "ab29a14e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ff4b0e6-b45b-4b8f-9d43-860568e53286"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 epoch-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 160/160 [06:17<00:00,  2.36s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], loss_G: 1.6130, loss_D_ABP: 0.1905, loss_D_PPG: 0.1724\n",
            "2 epoch-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 160/160 [06:15<00:00,  2.35s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/10], loss_G: 1.2822, loss_D_ABP: 0.1719, loss_D_PPG: 0.1593\n",
            "3 epoch-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 160/160 [06:16<00:00,  2.35s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/10], loss_G: 1.3330, loss_D_ABP: 0.1652, loss_D_PPG: 0.1489\n",
            "4 epoch-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 160/160 [06:15<00:00,  2.35s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/10], loss_G: 1.1347, loss_D_ABP: 0.1551, loss_D_PPG: 0.1304\n",
            "5 epoch-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 160/160 [06:15<00:00,  2.35s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/10], loss_G: 1.0801, loss_D_ABP: 0.1599, loss_D_PPG: 0.1338\n",
            "6 epoch-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 160/160 [06:15<00:00,  2.35s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/10], loss_G: 0.9516, loss_D_ABP: 0.1514, loss_D_PPG: 0.1255\n",
            "7 epoch-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 96%|█████████▋| 154/160 [06:02<00:14,  2.35s/it]"
          ]
        }
      ],
      "source": [
        "for epoch in range(epoch, n_epochs):\n",
        "    print(\"%d epoch-------------------------------\"%(epoch+1))\n",
        "    for real_ppg, real_abp in tqdm(loader_train):\n",
        "        time.sleep(0.01)\n",
        "        #########################################\n",
        "        # Generator PPG2ABP and ABP2PPG\n",
        "        # 1. GAN loss\n",
        "        # 2. Cycle loss\n",
        "        # 3. Identity loss\n",
        "        # 4. Total loss\n",
        "        #########################################\n",
        "        \n",
        "        # Forward------------------------------------------\n",
        "        # Transform the datasets\n",
        "        real_ppg_3d = real_ppg.unsqueeze(0).transpose(0,1).to(device)\n",
        "        real_abp_3d = real_abp.unsqueeze(0).transpose(0,1).to(device)\n",
        "        \n",
        "        # 1. Generate fake data\n",
        "        fake_abp = netG_P2A(real_ppg_3d)\n",
        "        fake_ppg = netG_A2P(real_abp_3d)\n",
        "\n",
        "        recovered_ppg = netG_A2P(fake_abp)\n",
        "        recovered_abp = netG_P2A(fake_ppg)\n",
        "        \n",
        "        identity_abp = netG_P2A(real_abp_3d)\n",
        "        identity_ppg = netG_A2P(real_ppg_3d)\n",
        "        \n",
        "        \n",
        "        # Backward generator path --------------------------\n",
        "        discFakeppg = netD_PPG(fake_ppg)\n",
        "        discFakeabp = netD_ABP(fake_abp)\n",
        "        discCycleppg = netD_PPG(recovered_ppg)\n",
        "        discCycleabp = netD_ABP(recovered_abp)\n",
        "        \n",
        "        \n",
        "        # 1. GAN loss\n",
        "        loss_ppg = criterion_gan(discFakeppg, torch.ones_like(discFakeppg))\n",
        "        loss_abp = criterion_gan(discFakeabp, torch.ones_like(discFakeabp))\n",
        "        \n",
        "        \n",
        "        # 2. Cycle loss\n",
        "        loss_cycle_abp = criterion_cycle(recovered_abp, real_abp_3d)\n",
        "        loss_cycle_ppg = criterion_cycle(recovered_ppg, real_ppg_3d)\n",
        "        \n",
        "        # 3. Identity loss\n",
        "        loss_identity_ppg = criterion_identity(identity_ppg, real_ppg_3d)\n",
        "        loss_identity_abp = criterion_identity(identity_abp, real_abp_3d)\n",
        "        \n",
        "        \n",
        "        # 4. Total loss = GAN loss + Cycle loss + Identity loss\n",
        "        loss_G = (loss_ppg + loss_abp) + \\\n",
        "                 (loss_identity_ppg + loss_identity_abp) * 5.0 + \\\n",
        "                 (loss_cycle_ppg + loss_cycle_abp) * 10.0\n",
        "        \n",
        "        optimizer_G.zero_grad()\n",
        "        loss_G.backward()   # Calculates the loss of the loss function\n",
        "        optimizer_G.step()\n",
        "        \n",
        "        \n",
        "        # Backward generator path --------------------------\n",
        "        \n",
        "        discFakeppg = netD_PPG(fake_ppg.detach())\n",
        "        discRealppg = netD_PPG(real_ppg_3d)\n",
        "        \n",
        "        loss_D_Real_ppg = criterion_gan(discFakeppg, torch.ones_like(discFakeppg))\n",
        "        loss_D_Fake_ppg = criterion_gan(discRealppg, torch.zeros_like(discRealppg))\n",
        "                        \n",
        "        loss_D_PPG = (loss_D_Real_ppg + loss_D_Fake_ppg) * 0.5\n",
        "        optimizer_D_PPG.zero_grad()\n",
        "        loss_D_PPG.requires_grad_(True)\n",
        "        loss_D_PPG.backward()\n",
        "        optimizer_D_PPG.step()\n",
        "        \n",
        "        discFakeabp = netD_ABP(fake_abp.detach())\n",
        "        discRealabp = netD_ABP(real_abp_3d)\n",
        "        \n",
        "        loss_D_Real_abp = criterion_gan(discFakeabp, torch.ones_like(discFakeabp))\n",
        "        loss_D_Fake_abp = criterion_gan(discRealabp, torch.zeros_like(discRealabp))\n",
        "        \n",
        "        loss_D_ABP = (loss_D_Real_abp + loss_D_Fake_abp) * 0.5\n",
        "        optimizer_D_ABP.zero_grad()\n",
        "        loss_D_ABP.requires_grad_(True)\n",
        "        loss_D_ABP.backward()\n",
        "        optimizer_D_ABP.step()\n",
        "        \n",
        "    # Print loss\n",
        "    print(\"Epoch [{}/{}], loss_G: {:.4f}, loss_D_ABP: {:.4f}, loss_D_PPG: {:.4f}\".format(\n",
        "           epoch+1, n_epochs, loss_G, loss_D_ABP, loss_D_PPG))\n",
        "\n",
        "    # Update learning rates\n",
        "    lr_scheduler_G.step()\n",
        "    lr_scheduler_D_PPG.step()\n",
        "    lr_scheduler_D_ABP.step()\n",
        "\n",
        "    # Save models checkpoints\n",
        "    if min_loss_G >= loss_G:\n",
        "      min_loss_G = loss_G\n",
        "      torch.save({\n",
        "            'epoch_info': epoch,\n",
        "            'netG_P2A_state_dict': netG_P2A.state_dict(),\n",
        "            'netG_A2P_state_dict': netG_A2P.state_dict(), \n",
        "            'netD_ABP_state_dict': netD_ABP.state_dict(), \n",
        "            'netD_PPG_state_dict': netD_PPG.state_dict(), \n",
        "            'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
        "            'optimizer_D_PPG_state_dict': optimizer_D_PPG.state_dict(),\n",
        "            'optimizer_D_ABP_state_dict': optimizer_D_ABP.state_dict(),\n",
        "            'criterion_gan_state_dict': criterion_gan.state_dict(),\n",
        "            'criterion_identity_state_dict': criterion_identity.state_dict(),\n",
        "            'criterion_cycle_state_dict': criterion_cycle.state_dict(),\n",
        "            'lr_scheduler_G_state_dict': lr_scheduler_G.state_dict(),\n",
        "            'lr_scheduler_D_PPG_state_dict': lr_scheduler_D_PPG.state_dict(),\n",
        "            'lr_scheduler_D_ABP_state_dict': lr_scheduler_D_ABP.state_dict()\n",
        "      }, 'model%s.pth'%(epoch))\n",
        "      "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(recovered_abp.shape, real_abp_3d.shape)\n",
        "print(recovered_ppg.shape, real_ppg_3d.shape)"
      ],
      "metadata": {
        "id": "FmERTp4aFEDZ"
      },
      "id": "FmERTp4aFEDZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "506ee437",
      "metadata": {
        "id": "506ee437"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fd52978",
      "metadata": {
        "id": "4fd52978"
      },
      "outputs": [],
      "source": [
        "###### Test ######\n",
        "\n",
        "# Create output dirs if they don't exist\n",
        "dataNum = 1\n",
        "for real_ppg, real_abp in loader_test:\n",
        "    # Set model input\n",
        "    real_ppg_3d = real_ppg.unsqueeze(0).transpose(0,1).to(device)\n",
        "    real_abp_3d = real_abp.unsqueeze(0).transpose(0,1).to(device)\n",
        "\n",
        "    # Generate output\n",
        "    fake_abp = netG_P2A(real_ppg_3d).data\n",
        "    fake_ppg = netG_A2P(real_abp_3d).data\n",
        "    \n",
        "    # Find min-max value from test datasets\n",
        "    max_ppg, min_ppg = findMinMax(ppg_test)[0], findMinMax(ppg_test)[1]\n",
        "    max_abp, min_abp = findMinMax(abp_test)[0], findMinMax(abp_test)[1]\n",
        "\n",
        "    # Revert the data from normalized one\n",
        "    revert_real_abp = real_abp_3d * (max_abp - min_abp) + min_abp\n",
        "    revert_real_ppg = real_ppg_3d * (max_ppg - min_ppg) + min_ppg\n",
        "    revert_fake_abp = fake_abp * (max_abp - min_abp) + min_abp\n",
        "    revert_fake_ppg = fake_ppg * (max_ppg - min_ppg) + min_ppg\n",
        "\n",
        "    # Save data file \n",
        "    resultfile = opt.output + 'data%d.npz'%(dataNum)\n",
        "    np.savez(resultfile, real_abp=revert_real_abp.squeeze().cpu(), fake_abp=revert_fake_abp.squeeze().cpu(), \n",
        "             real_ppg=revert_real_ppg.squeeze().cpu(), fake_ppg=revert_fake_ppg.squeeze().cpu()) # Save cahce file\n",
        "    dataNum += 1\n",
        "\n",
        "\n",
        "# Make a graph with index 0 dataset\n",
        "graph(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "023a9ea0",
      "metadata": {
        "id": "023a9ea0"
      },
      "source": [
        "## Draw a graph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = np.load('/content/drive/MyDrive/snuh/output/ShuffleTrue/dataNum1-shuffleFalse.npz')\n",
        "real_abp_sets = result['real_abp']\n",
        "real_ppg_sets = result['real_ppg']\n",
        "\n",
        "fake_abp_sets = result['fake_abp']\n",
        "fake_ppg_sets = result['fake_ppg']\n",
        "\n",
        "plt.figure(figsize=(20,5))\n",
        "plt.plot(real_abp_sets[0], color='g', label='ground-truth ABP')\n",
        "plt.plot(fake_abp_sets[0], color='r', label='Generated ABP')\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(20,5))\n",
        "plt.plot(real_ppg_sets[0], color='g', label='ground-truth PPG')\n",
        "plt.plot(fake_ppg_sets[0], color='r', label='Generated PPG')\n",
        "plt.legend()\n",
        "\n",
        "plt.savefig('/content/drive/MyDrive/snuh/output/ShuffleTrue/realVSfake-dataNum1.png')"
      ],
      "metadata": {
        "id": "AQUBLq5abX8f"
      },
      "id": "AQUBLq5abX8f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0,10):\n",
        "  plt.figure(figsize=(20,5))\n",
        "  plt.plot(real_abp_sets[i], color='g', label='ground-truth ABP')\n",
        "  plt.plot(fake_abp_sets[i], color='r', label='Generated ABP')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "plt.savefig('/content/drive/MyDrive/snuh/output/ShuffleTrue/realVSfake-dataNum1-10_onlyABP.png')"
      ],
      "metadata": {
        "id": "qHw2rQ1ebaL7"
      },
      "id": "qHw2rQ1ebaL7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.signal import savgol_filter\n",
        "fake_filtered_abp = savgol_filter(fake_abp_sets, 31, 3)\n",
        "\n",
        "for i in range(10):\n",
        "  plt.figure(figsize=(20,5))\n",
        "  plt.plot(real_abp_sets[i], color='g', label='real_ABP')\n",
        "  plt.plot(fake_filtered_abp[i], color='r', label='fake_ABP')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "plt.savefig('/content/drive/MyDrive/snuh/output/ShuffleTrue/realVSfake-dataNum1-10_onlyABP_after_filter.png')"
      ],
      "metadata": {
        "id": "js35l0Pnr1AN"
      },
      "id": "js35l0Pnr1AN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "bd82a9ec",
      "metadata": {
        "id": "bd82a9ec"
      },
      "source": [
        "## Reference"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d98581e",
      "metadata": {
        "id": "2d98581e"
      },
      "source": [
        "- https://github.com/aitorzip/PyTorch-CycleGAN\n",
        "- https://github.com/aitorzip/PyTorch-CycleGAN/blob/master/train\n",
        "- https://medium.com/humanscape-tech/ml-practice-cyclegan-f9153ef72297    \n",
        "- https://github.com/heejipark/Research-PPG2ABP-using-GAN/blob/master/paper-review/PPG2ECG_CardioGAN.md\n",
        "- https://github.com/heejipark/PyTorch-GAN/blob/36d3c77e5ff20ebe0aeefd322326a134a279b93e/implementations/cyclegan/cyclegan.py\n",
        "- https://www.slideshare.net/WuhyunRicoShin/tutorial-image-generation-and-imagetoimage-translation-using-gan\n",
        "- https://github.com/heejipark/PyTorch-GAN/blob/36d3c77e5ff20ebe0aeefd322326a134a279b93e/implementations/cyclegan/models.py#L22\n",
        "- https://medium.com/humanscape-tech/ml-practice-cyclegan-f9153ef72297\n",
        "- https://github.com/aitorzip/PyTorch-CycleGAN/blob/master/train"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f5221ed",
      "metadata": {
        "id": "4f5221ed"
      },
      "source": [
        "## nn.Conv()\n",
        "\n",
        "#### Conv1d — Input 1d\n",
        "<pre>\n",
        "nn.Conv1d() applies 1D convolution over the input. nn.Conv1d() expects the input to be of the shape [batch_size, input_channels, signal_length] .\n",
        "\n",
        "You can check out the complete list of parameters in the official PyTorch Docs. The required parameters are —\n",
        "\n",
        "in_channels (python:int) — Number of channels in the input signal. This should be equal to the number of channels in the input tensor.\n",
        "out_channels (python:int) — Number of channels produced by the convolution.\n",
        "kernel_size (python:int or tuple) — Size of the convolving kernel.\n",
        "</pre>\n",
        "<code>\n",
        "  input_1d = tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]) = torch.Size([10])\n",
        "  input_2d = tensor([[ 1.,  2.,  3.,  4.,  5.], [ 6.,  7.,  8.,  9., 10.]]) = torch.Size([2, 5])\n",
        "  \n",
        "  input_1d = input_1d.unsqueeze(0).unsqueeze(0) # in order to insert CNN, change the format\n",
        "  input_2d = input_2d.unsqueeze(0)\n",
        "</code>\n",
        "\n",
        "\n",
        "#### Conv1d — Input 2d\n",
        "<pre>\n",
        "To apply 1D convolution on a 2d input signal, we can do the following. First, we define our input tensor of the size [1, 2, 5] where batch_size = 1,\n",
        "\n",
        "</pre>\n",
        "\n",
        "\n",
        "#### Conv2d - Input 3d\n",
        "nn.Conv2d() applies 2D convolution over the input. nn.Conv2d() expects the input to be of the shape [batch_size, input_channels, input_height,\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "reference: https://towardsdatascience.com/pytorch-basics-how-to-train-your-neural-net-intro-to-cnn-26a14c2ea29"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "name": "edit",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}